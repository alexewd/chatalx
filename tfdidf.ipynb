{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6cd2f628-bd19-420c-a66c-47d0e4bc868c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "93bc3153-793c-4cf2-ba80-7784ec6ed434",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "34a5f372-fbc5-49b4-b86a-fea29739cfc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Загрузка текстов из папки\n",
    "def load_texts_from_folder(folder_path):\n",
    "    texts = []\n",
    "    for file_path in Path(folder_path).glob(\"*.txt\"):\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            texts.append(f.read())\n",
    "    return texts\n",
    "\n",
    "\n",
    "# Основной процесс\n",
    "folder_path = Path(\"e:/cleaned_text\")\n",
    "texts = load_texts_from_folder(folder_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "79a00f0f-08bd-498e-af2e-254f124e5586",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Путь к текстовому файлу\n",
    "# text_file = Path(\"e:/cleaned_json_letter/Лесков Николай. Том 11.txt\")\n",
    "\n",
    "# # Функция для разбиения текста на предложения\n",
    "# def split_into_sentences(text):\n",
    "#     # Регулярное выражение для нахождения предложений\n",
    "#     sentence_endings = r\"[.!?]\\\\s\"\n",
    "#     sentences = re.split(sentence_endings, text)\n",
    "#     # Убираем пустые строки и лишние пробелы\n",
    "#     return [sentence.strip() for sentence in sentences if sentence.strip()]\n",
    "\n",
    "# # Читаем текстовый файл\n",
    "# with open(text_file, \"r\", encoding=\"utf-8\") as file:\n",
    "#     text = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f276eb84-3d48-42cd-9257-a5bf8c04fd65",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\alexewd\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "expected string or bytes-like object, got 'list'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 11\u001b[0m\n\u001b[0;32m      3\u001b[0m russian_stopwords \u001b[38;5;241m=\u001b[39m stopwords\u001b[38;5;241m.\u001b[39mwords(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrussian\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# # Разбиение текста на предложения\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# def split_into_sentences(text):\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m#     sentences = re.split(r'[.!?]', text)\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m#     return [sentence.strip() for sentence in sentences if sentence.strip()]\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# Разбиваем текст на предложения\u001b[39;00m\n\u001b[1;32m---> 11\u001b[0m sentences_list \u001b[38;5;241m=\u001b[39m \u001b[43msplit_into_sentences\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# Преобразуем список в массив NumPy\u001b[39;00m\n\u001b[0;32m     14\u001b[0m text_data \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(sentences_list)\n",
      "Cell \u001b[1;32mIn[6], line 7\u001b[0m, in \u001b[0;36msplit_into_sentences\u001b[1;34m(text)\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msplit_into_sentences\u001b[39m(text):\n\u001b[1;32m----> 7\u001b[0m     sentences \u001b[38;5;241m=\u001b[39m \u001b[43mre\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m[.!?]\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      8\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [sentence\u001b[38;5;241m.\u001b[39mstrip() \u001b[38;5;28;01mfor\u001b[39;00m sentence \u001b[38;5;129;01min\u001b[39;00m sentences \u001b[38;5;28;01mif\u001b[39;00m sentence\u001b[38;5;241m.\u001b[39mstrip()]\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\re\\__init__.py:206\u001b[0m, in \u001b[0;36msplit\u001b[1;34m(pattern, string, maxsplit, flags)\u001b[0m\n\u001b[0;32m    198\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msplit\u001b[39m(pattern, string, maxsplit\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, flags\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m):\n\u001b[0;32m    199\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Split the source string by the occurrences of the pattern,\u001b[39;00m\n\u001b[0;32m    200\u001b[0m \u001b[38;5;124;03m    returning a list containing the resulting substrings.  If\u001b[39;00m\n\u001b[0;32m    201\u001b[0m \u001b[38;5;124;03m    capturing parentheses are used in pattern, then the text of all\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    204\u001b[0m \u001b[38;5;124;03m    and the remainder of the string is returned as the final element\u001b[39;00m\n\u001b[0;32m    205\u001b[0m \u001b[38;5;124;03m    of the list.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 206\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _compile(pattern, flags)\u001b[38;5;241m.\u001b[39msplit(string, maxsplit)\n",
      "\u001b[1;31mTypeError\u001b[0m: expected string or bytes-like object, got 'list'"
     ]
    }
   ],
   "source": [
    "# Убедимся, что стоп-слова загружены\n",
    "nltk.download(\"stopwords\")\n",
    "russian_stopwords = stopwords.words(\"russian\")\n",
    "\n",
    "# Разбиение текста на предложения\n",
    "def split_into_sentences(text):\n",
    "    sentences = re.split(r'[.!?]', text)\n",
    "    return [sentence.strip() for sentence in sentences if sentence.strip()]\n",
    "\n",
    "# Разбиваем текст на предложения\n",
    "sentences_list = split_into_sentences(texts)\n",
    "\n",
    "# Преобразуем список в массив NumPy\n",
    "text_data = np.array(sentences_list)\n",
    "\n",
    "# Пример использования CountVectorizer\n",
    "count = CountVectorizer()\n",
    "bag_of_words = count.fit_transform(text_data)\n",
    "\n",
    "# Используем TfidfVectorizer с фильтрацией стоп-слов и биграммами\n",
    "tfidf = TfidfVectorizer(ngram_range=(1, 2), stop_words=russian_stopwords)\n",
    "feature_matrix = tfidf.fit_transform(text_data)\n",
    "\n",
    "# Получаем слова и биграммы с их значениями TF-IDF\n",
    "feature_names = tfidf.get_feature_names_out()\n",
    "tfidf_scores = feature_matrix.sum(axis=0).A1\n",
    "\n",
    "# Создаем список из слов/биграмм и их значений\n",
    "features_with_scores = list(zip(feature_names, tfidf_scores))\n",
    "\n",
    "# Сортируем по убыванию значимости\n",
    "sorted_features = sorted(features_with_scores, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Фильтруем элементы: убираем цифры и комбинации букв и цифр\n",
    "filtered_features = [(feature, score) for feature, score in sorted_features if not re.search(r\"\\\\d\", feature)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aef4cfe-d2aa-4266-8e5f-65633eb6c14e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Выводим топ-10 самых значимых слов и биграмм\n",
    "print(\"Топ-10 значимых слов и биграмм по TF-IDF:\")\n",
    "for feature, score in filtered_features[:10]:\n",
    "    print(f\"{feature}: {score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3253c15e-a086-497d-82ae-d7018f61182c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Выводим примерный результат\n",
    "print(\"Пример предложений:\", text_data[:5])\n",
    "print(\"Форма bag_of_words:\", bag_of_words.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "002058e2-0616-43fa-afe3-8131041854b9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "add04958-5489-4287-b5bd-ccbe5ddf887c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "\n",
    "# Убедимся, что стоп-слова загружены\n",
    "# nltk.download(\"stopwords\")\n",
    "# russian_stopwords = stopwords.words(\"russian\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e46d2972-d532-48e1-932a-4af2bd7e52cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\alexewd\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Загрузка стоп-слов\n",
    "nltk.download(\"stopwords\")\n",
    "russian_stopwords = set(stopwords.words(\"russian\"))\n",
    "custom_stopwords = {\"сноска\",\"и\", \"в\", \"во\",\"и\", \"в\", \"во\", \"не\", \"что\", \"он\", \"на\", \"я\", \\\n",
    "\"с\", \"со\", \"как\", \"а\", \"то\", \"все\", \"она\", \"так\", \"его\", \"но\", \"да\", \"ты\", \"к\", \"у\", \"же\", \"вы\", \"за\",\\\n",
    " \"бы\", \"по\", \"только\", \"ее\", \"мне\", \"было\", \"вот\", \"от\", \"меня\", \"еще\", \"нет\", \"о\", \"из\", \"ему\", \"теперь\",\\\n",
    "   \"когда\", \"даже\", \"ну\", \"ли\", \"если\", \"уже\", \"или\", \"ни\", \"быть\", \"был\", \"него\", \"до\", \"вас\",\\\n",
    "     \"нибудь\", \"опять\", \"уж\", \"вам\", \"ведь\", \"там\", \"потом\", \"себя\", \"ничего\", \"ей\", \"может\", \"они\", \\\n",
    "     \"тут\", \"где\", \"есть\", \"надо\", \"ней\", \"для\", \"мы\", \"тебя\", \"их\", \"чем\", \"была\", \"сам\", \"чтоб\", \\\n",
    "     \"без\", \"будто\", \"чего\", \"раз\", \"тоже\", \"себе\", \"под\", \"лесков\", \"будет\", \"ж\", \"тогда\", \"кто\", \\\n",
    "     \"этот\", \"говорил\", \"того\", \"потому\", \"этого\", \"какой\", \"совсем\", \"ним\", \"здесь\", \"этом\",\\\n",
    "       \"один\", \"почти\", \"мой\", \"тем\", \"чтобы\", \"нее\", \"кажется\", \"сейчас\", \"были\", \"куда\",\\\n",
    "         \"зачем\", \"сказать\", \"всех\", \"никогда\", \"можно\", \"при\", \"наконец\", \"два\",\\\n",
    "           \"об\", \"хоть\", \"после\", \"над\", \"тот\", \"через\", \"эти\", \"нас\", \"про\",\\\n",
    "             \"них\", \"какая\", \"много\", \"разве\", \"три\", \"эту\", \"моя\", \"впрочем\", \"свою\",\\\n",
    "               \"этой\", \"перед\", \"иногда\", \"лучше\", \"чуть\", \"том\", \"нельзя\", \"такой\", \"им\", \"более\",\\\n",
    "                 \"всегда\", \"конечно\", \"всю\",\"это\",\\\n",
    "                   \"не\", \"что\", \"он\", \"на\", \"я\", \"с\",\\\n",
    "                     \"со\", \"как\", \"н\", \"то\", \"все\", \"она\", \"так\", \"г\", \"очень\", \"стр\", \"ты\", \"к\", \\\n",
    "                     \"у\", \"же\", \"вы\", \"за\", \"бы\", \"по\", \"только\", \"ее\", \"мне\", \"было\", \"вот\", \"от\",\\\n",
    "                       \"меня\", \"еще\", \"нет\", \"о\", \"из\", \"ему\", \"теперь\", \"когда\", \"даже\", \"ну\", \"вдруг\",\\\n",
    "                         \"ли\", \"если\", \"уже\", \"или\", \"ни\", \"быть\", \"был\", \"него\", \"до\", \"вас\",\\\n",
    "                           \"опять\", \"уж\", \"вам\", \"ведь\", \"там\", \"потом\", \"себя\", \"ничего\", \"ей\", \\\n",
    "                           \"может\", \"они\", \"тут\", \"где\", \"есть\", \"надо\", \"ней\", \"для\", \"мы\", \"тебя\",\\\n",
    "                             \"их\", \"чем\", \"была\", \"сам\", \"чтоб\", \"без\", \"будто\", \"чего\", \"раз\", \"тоже\",\\\n",
    "                               \"себе\", \"под\", \"будет\", \"ж\", \"тогда\", \"кто\", \"этот\",\\\n",
    "                                 \"того\", \"потому\", \"этого\", \"какой\", \"совсем\", \"ним\", \"здесь\", \"этом\", \\\n",
    "                                 \"почти\", \"мой\", \"тем\", \"чтобы\", \"нее\", \"кажется\", \\\n",
    "                                   \"были\", \"куда\", \"зачем\", \"сказать\", \"всех\", \"никогда\",  \\\n",
    "                                   \"можно\", \"при\", \"наконец\", \"два\", \"об\", \"другой\", \"хоть\", \"после\",\\\n",
    "                                     \"над\", \"больше\", \"тот\", \"через\", \"эти\", \"нас\", \"про\", \"них\", \"какая\",\\\n",
    "                                       \"много\", \"разве\", \"три\", \"эту\", \"моя\", \"впрочем\", \"свою\",\\\n",
    "                                         \"этой\", \"перед\", \"иногда\", \"чуть\", \"том\",\\\n",
    "                                           \"такой\", \"им\", \"более\", \"конечно\"}\n",
    "\n",
    "russian_stopwords.update(custom_stopwords)\n",
    "\n",
    "# Убедимся, что стоп-слова преобразованы в список\n",
    "russian_stopwords = list(russian_stopwords)\n",
    "\n",
    "# Используем TfidfVectorizer\n",
    "# tfidf = TfidfVectorizer(ngram_range=(1, 2), stop_words=russian_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ca1c274d-7f70-4c71-a0b4-52b928865e27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Топ-10 значимых слов и биграмм по TF-IDF:\n",
      "говорит: 722.92\n",
      "такое: 569.58\n",
      "говорю: 567.09\n",
      "дело: 480.56\n",
      "тебе: 468.50\n",
      "знаю: 460.12\n",
      "время: 429.13\n",
      "сказал: 400.75\n",
      "отвечал: 385.82\n",
      "человек: 382.96\n",
      "см: 360.29\n",
      "отчего: 350.67\n",
      "нам: 321.75\n",
      "люди: 321.08\n",
      "который: 318.42\n",
      "лескова: 314.22\n",
      "нем: 304.24\n",
      "могу: 304.12\n",
      "всем: 303.61\n",
      "этим: 303.29\n",
      "спросил: 293.85\n",
      "делать: 277.66\n",
      "своей: 276.52\n",
      "прим: 274.53\n",
      "однако: 271.31\n",
      "франц: 271.01\n",
      "мог: 270.49\n",
      "людей: 267.94\n",
      "которые: 260.98\n",
      "своего: 259.28\n",
      "отец: 256.46\n",
      "думаю: 252.01\n",
      "лиза: 250.78\n",
      "ах: 241.34\n",
      "почему: 238.50\n",
      "письмо: 237.39\n",
      "говорят: 237.06\n",
      "кого: 234.63\n",
      "разумеется: 234.51\n",
      "просто: 233.10\n",
      "Форма bag_of_words: (141668, 164543)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Функция для разбиения текста на предложения\n",
    "def split_into_sentences(text):\n",
    "    # Регулярное выражение для нахождения предложений\n",
    "    sentence_endings = r\"[.!?]\\s\"\n",
    "    sentences = re.split(sentence_endings, text)\n",
    "    # Убираем пустые строки и лишние пробелы\n",
    "    return [sentence.strip() for sentence in sentences if sentence.strip()]\n",
    "\n",
    "\n",
    "def load_texts_from_folder(folder_path):\n",
    "    texts = []\n",
    "    for file_path in Path(folder_path).glob(\"*.txt\"):\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            texts.append(f.read())\n",
    "    return texts\n",
    "\n",
    "\n",
    "# Основной процесс\n",
    "folder_path = Path(\"e:/cleaned_text\")\n",
    "texts = load_texts_from_folder(folder_path)\n",
    "\n",
    "# Разбиваем тексты на предложения и создаём один общий список предложений\n",
    "all_sentences = []\n",
    "for text in texts:\n",
    "    all_sentences.extend(split_into_sentences(text))\n",
    "\n",
    "# Преобразуем список предложений в массив NumPy\n",
    "text_data = np.array(all_sentences)\n",
    "\n",
    "# Пример использования CountVectorizer\n",
    "count = CountVectorizer()\n",
    "bag_of_words = count.fit_transform(text_data)\n",
    "\n",
    "# Используем TfidfVectorizer с фильтрацией стоп-слов и биграммами\n",
    "# tfidf = TfidfVectorizer(ngram_range=(1, 2), stop_words=russian_stopwords)\n",
    "# feature_matrix = tfidf.fit_transform(text_data)\n",
    "\n",
    "tfidf = TfidfVectorizer(\n",
    "    ngram_range=(1, 2),  # Односложные слова и биграммы\n",
    "    stop_words=russian_stopwords,  # Удаляем стоп-слова\n",
    "    max_df=0.8,  # Убираем слишком частые слова\n",
    "    min_df=5,    # Убираем слишком редкие слова\n",
    "    norm=\"l2\"    # Применяем нормализацию\n",
    ")\n",
    "\n",
    "# tfidf = TfidfVectorizer(ngram_range=(1, 2), stop_words=russian_stopwords)\n",
    "\n",
    "feature_matrix = tfidf.fit_transform(text_data)\n",
    "\n",
    "\n",
    "# Получаем слова и биграммы с их значениями TF-IDF\n",
    "feature_names = tfidf.get_feature_names_out()\n",
    "tfidf_scores = feature_matrix.sum(axis=0).A1\n",
    "\n",
    "# Создаем список из слов/биграмм и их значений\n",
    "features_with_scores = list(zip(feature_names, tfidf_scores))\n",
    "\n",
    "# Сортируем по убыванию значимости\n",
    "sorted_features = sorted(features_with_scores, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Фильтруем элементы: убираем цифры и комбинации букв и цифр\n",
    "filtered_features = [(feature, score) for feature, score in sorted_features if not re.search(r\"\\d\", feature)]\n",
    "\n",
    "# Выводим топ-10 самых значимых слов и биграмм\n",
    "print(\"Топ-10 значимых слов и биграмм по TF-IDF:\")\n",
    "for feature, score in filtered_features[:40]:\n",
    "    print(f\"{feature}: {score:.2f}\")\n",
    "\n",
    "print(\"Форма bag_of_words:\", bag_of_words.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "31c2b6a7-38d0-40ea-b481-00e2b98689ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Топ-10 значимых биграмм и триграмм по TF-IDF:\n",
      "публикуется впервые: 229.40\n",
      "печатается автографу: 124.55\n",
      "самом деле: 85.64\n",
      "жизнь николая: 75.94\n",
      "жизнь николая лескова: 75.94\n",
      "николая лескова: 75.94\n",
      "автографу ирли: 72.81\n",
      "печатается автографу ирли: 72.34\n",
      "впервые опубликовано: 64.39\n",
      "собрание сочинений: 60.21\n",
      "примечание письму: 48.22\n",
      "лизавета егоровна: 45.69\n",
      "крайней мере: 40.01\n",
      "сих пор: 38.29\n",
      "шестидесятые годы: 36.26\n",
      "имеется виду: 35.76\n",
      "катерина львовна: 35.44\n",
      "домна платоновна: 35.00\n",
      "тех пор: 32.59\n",
      "речь идет: 31.20\n",
      "впервые опубликовано сборнике: 31.00\n",
      "опубликовано сборнике: 31.00\n",
      "друг друга: 29.97\n",
      "черт знает: 29.84\n",
      "марфа андревна: 29.76\n",
      "впервые опубликовано книге: 28.82\n",
      "опубликовано книге: 28.82\n",
      "автографу цгали: 28.48\n",
      "бог знает: 28.34\n",
      "печатается автографу цгали: 28.18\n",
      "анна семеновна: 28.01\n",
      "печатается тексту: 27.99\n",
      "евгения петровна: 26.95\n",
      "таким образом: 25.41\n",
      "алексей сергеевич: 24.66\n",
      "спросил розанов: 24.54\n",
      "слава богу: 24.19\n",
      "фирс григорьич: 24.16\n",
      "ида ивановна: 24.05\n",
      "сборнике шестидесятые: 23.88\n"
     ]
    }
   ],
   "source": [
    "# Используем TfidfVectorizer для биграмм и триграмм\n",
    "tfidf_ngrams = TfidfVectorizer(ngram_range=(2, 3), stop_words=russian_stopwords)\n",
    "ngram_feature_matrix = tfidf_ngrams.fit_transform(text_data)\n",
    "\n",
    "# Получаем биграммы и триграммы с их значениями TF-IDF\n",
    "ngram_feature_names = tfidf_ngrams.get_feature_names_out()\n",
    "ngram_tfidf_scores = ngram_feature_matrix.sum(axis=0).A1\n",
    "\n",
    "# Создаем список из биграмм/триграмм и их значений\n",
    "ngram_features_with_scores = list(zip(ngram_feature_names, ngram_tfidf_scores))\n",
    "\n",
    "# Сортируем по убыванию значимости\n",
    "sorted_ngram_features = sorted(ngram_features_with_scores, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Фильтруем биграммы и триграммы (убираем цифры и другие ненужные элементы)\n",
    "filtered_ngram_features = [\n",
    "    (feature, score) for feature, score in sorted_ngram_features if not re.search(r\"\\d\", feature)\n",
    "]\n",
    "\n",
    "# Выводим топ-10 биграмм и триграмм\n",
    "print(\"\\nТоп-10 значимых биграмм и триграмм по TF-IDF:\")\n",
    "for feature, score in filtered_ngram_features[:40]:\n",
    "    print(f\"{feature}: {score:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4bee4e1d-d7d4-4a2f-8c4e-399bea9cb373",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Используем TfidfVectorizer для монограмм\n",
    "tfidf_monograms = TfidfVectorizer(ngram_range=(1, 1), stop_words=russian_stopwords)\n",
    "monogram_feature_matrix = tfidf_monograms.fit_transform(text_data)\n",
    "\n",
    "# Получаем монограммы и их значения TF-IDF\n",
    "monogram_feature_names = tfidf_monograms.get_feature_names_out()\n",
    "monogram_tfidf_scores = monogram_feature_matrix.sum(axis=0).A1\n",
    "\n",
    "# Создаем список из монограмм и их значений\n",
    "monogram_features_with_scores = list(zip(monogram_feature_names, monogram_tfidf_scores))\n",
    "\n",
    "# Создаем DataFrame для монограмм\n",
    "monogram_df = pd.DataFrame(monogram_features_with_scores, columns=[\"Monogram\", \"TF-IDF\"])\n",
    "monogram_df[\"Frequency\"] = monogram_df[\"TF-IDF\"].apply(lambda x: x * 100)  # Примерная частота на основе TF-IDF\n",
    "\n",
    "# Округляем значения до 2 знаков после запятой\n",
    "monogram_df[\"TF-IDF\"] = monogram_df[\"TF-IDF\"].round(2)\n",
    "monogram_df[\"Frequency\"] = monogram_df[\"Frequency\"].round(2)\n",
    "\n",
    "monogram_df = monogram_df.sort_values(by=\"TF-IDF\", ascending=False).reset_index(drop=True)\n",
    "\n",
    "# Используем TfidfVectorizer для биграмм и триграмм\n",
    "tfidf_ngrams = TfidfVectorizer(ngram_range=(2, 3), stop_words=russian_stopwords)\n",
    "ngram_feature_matrix = tfidf_ngrams.fit_transform(text_data)\n",
    "\n",
    "# Получаем биграммы и триграммы с их значениями TF-IDF\n",
    "ngram_feature_names = tfidf_ngrams.get_feature_names_out()\n",
    "ngram_tfidf_scores = ngram_feature_matrix.sum(axis=0).A1\n",
    "\n",
    "# Создаем список из биграмм и триграмм и их значений\n",
    "ngram_features_with_scores = list(zip(ngram_feature_names, ngram_tfidf_scores))\n",
    "\n",
    "# Фильтруем биграммы и триграммы (убираем цифры)\n",
    "filtered_ngram_features = [\n",
    "    (feature, score) for feature, score in ngram_features_with_scores if not re.search(r\"\\d\", feature)\n",
    "]\n",
    "\n",
    "# Разделяем на биграммы и триграммы\n",
    "bigram_features = [(feature, score) for feature, score in filtered_ngram_features if len(feature.split()) == 2]\n",
    "trigram_features = [(feature, score) for feature, score in filtered_ngram_features if len(feature.split()) == 3]\n",
    "\n",
    "# Создаем DataFrame для биграмм\n",
    "bigram_df = pd.DataFrame(bigram_features, columns=[\"Bigram\", \"TF-IDF\"])\n",
    "bigram_df[\"Frequency\"] = bigram_df[\"TF-IDF\"].apply(lambda x: x * 100)  # Примерная частота\n",
    "\n",
    "# Округляем значения до 2 знаков после запятой\n",
    "bigram_df[\"TF-IDF\"] = bigram_df[\"TF-IDF\"].round(2)\n",
    "bigram_df[\"Frequency\"] = bigram_df[\"Frequency\"].round(2)\n",
    "\n",
    "bigram_df = bigram_df.sort_values(by=\"TF-IDF\", ascending=False).reset_index(drop=True)\n",
    "\n",
    "# Создаем DataFrame для триграмм\n",
    "trigram_df = pd.DataFrame(trigram_features, columns=[\"Trigram\", \"TF-IDF\"])\n",
    "trigram_df[\"Frequency\"] = trigram_df[\"TF-IDF\"].apply(lambda x: x * 100)  # Примерная частота\n",
    "\n",
    "# Округляем значения до 2 знаков после запятой\n",
    "trigram_df[\"TF-IDF\"] = trigram_df[\"TF-IDF\"].round(2)\n",
    "trigram_df[\"Frequency\"] = trigram_df[\"Frequency\"].round(2)\n",
    "\n",
    "trigram_df = trigram_df.sort_values(by=\"TF-IDF\", ascending=False).reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "66ff6b54-a94f-438d-bbe8-8cb242153909",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Монограммы:\n",
      "   Monogram  TF-IDF  Frequency\n",
      "0   говорит  665.54   66554.06\n",
      "1     такое  554.60   55459.94\n",
      "2    говорю  540.90   54089.75\n",
      "3      дело  457.10   45710.44\n",
      "4      знаю  452.30   45230.08\n",
      "5      тебе  449.54   44953.81\n",
      "6     время  392.09   39208.91\n",
      "7   отвечал  377.47   37746.52\n",
      "8    сказал  374.22   37421.68\n",
      "9   человек  357.34   35733.60\n",
      "10       см  335.70   33569.62\n",
      "11   отчего  334.58   33458.33\n",
      "12  лескова  329.00   32900.01\n",
      "13     могу  300.05   30004.79\n",
      "14      нам  290.78   29077.67\n",
      "15     люди  290.64   29063.97\n",
      "16  спросил  290.08   29008.28\n",
      "17  который  283.80   28380.10\n",
      "18   делать  273.53   27353.03\n",
      "19     всем  273.20   27319.52\n"
     ]
    }
   ],
   "source": [
    "# Выводим таблицы\n",
    "print(\"Монограммы:\")\n",
    "print(monogram_df.head(20))  # Топ-10 монограмм"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "31d58e42-1744-4696-8a9c-44f7bd65f146",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Биграммы:\n",
      "                   Bigram  TF-IDF  Frequency\n",
      "0     публикуется впервые  229.40   22939.59\n",
      "1    печатается автографу  124.55   12455.21\n",
      "2              самом деле   85.64    8563.52\n",
      "3           жизнь николая   75.94    7593.77\n",
      "4         николая лескова   75.94    7593.77\n",
      "5          автографу ирли   72.81    7281.25\n",
      "6    впервые опубликовано   64.39    6438.63\n",
      "7      собрание сочинений   60.21    6021.29\n",
      "8       примечание письму   48.22    4821.99\n",
      "9       лизавета егоровна   45.69    4569.05\n",
      "10           крайней мере   40.01    4000.58\n",
      "11                сих пор   38.29    3829.01\n",
      "12      шестидесятые годы   36.26    3625.90\n",
      "13           имеется виду   35.76    3575.99\n",
      "14       катерина львовна   35.44    3544.08\n",
      "15       домна платоновна   35.00    3499.63\n",
      "16                тех пор   32.59    3259.08\n",
      "17              речь идет   31.20    3119.94\n",
      "18  опубликовано сборнике   31.00    3100.26\n",
      "19             друг друга   29.97    2996.64\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nБиграммы:\")\n",
    "print(bigram_df.head(20))  # Топ-10 биграмм"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "4e44d4bd-7a47-4c95-a658-79e2bbd70a76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Триграммы:\n",
      "                               Trigram  TF-IDF  Frequency\n",
      "0                жизнь николая лескова   75.94    7593.77\n",
      "1            печатается автографу ирли   72.34    7234.28\n",
      "2        впервые опубликовано сборнике   31.00    3100.26\n",
      "3           впервые опубликовано книге   28.82    2882.17\n",
      "4           печатается автографу цгали   28.18    2817.53\n",
      "5           сборнике шестидесятые годы   23.88    2387.62\n",
      "6             печатается автографу гпб   22.97    2296.86\n",
      "7   опубликовано сборнике шестидесятые   22.85    2285.45\n",
      "8            полное собрание сочинений   19.42    1941.96\n",
      "9                   ваш покорный слуга   13.55    1354.84\n",
      "10         уважаемый алексей сергеевич    9.81     981.26\n",
      "11             уважаемый петр карлович    9.73     972.76\n",
      "12    печатается автографу толстовский    9.69     968.73\n",
      "13         автографу толстовский музей    9.69     968.73\n",
      "14            сборнике письма толстого    9.39     938.62\n",
      "15        опубликовано сборнике письма    9.39     938.62\n",
      "16                            ха ха ха    9.01     901.08\n",
      "17             печатается автографу лб    8.93     893.38\n",
      "18            говорит домна платоновна    8.20     820.25\n",
      "19        впервые опубликовано журнале    7.81     781.29\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nТриграммы:\")\n",
    "print(trigram_df.head(20))  # Топ-10 триграмм"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "500d8697-7f87-4bf6-95b2-23e58593c075",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\alexewd\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['', 'иностранных', 'иностранных слов', 'руб', 'рублей', 'рублей дохода', 'экземпляров', 'экземпляров концы', '', '', '', '', '', '', '', 'будут', 'затем', 'листов', 'подписью', '', 'гродно', '', 'лист', 'руб', '', 'годах', 'мин', '', 'мая', 'августа', 'августа', 'августа', 'августа буду', 'августа выехал', 'апреля', 'апреля', 'библейского', 'библейского поэта', 'благонамеренная', 'благонамеренная бестактность']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "\n",
    "# Загрузка текстов из папки\n",
    "def load_texts_from_folder(folder_path):\n",
    "    texts = []\n",
    "    for file_path in Path(folder_path).glob(\"*.txt\"):\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            texts.append(f.read())\n",
    "    return texts\n",
    "\n",
    "# Убедимся, что стоп-слова загружены\n",
    "nltk.download(\"stopwords\")\n",
    "russian_stopwords = stopwords.words(\"russian\")\n",
    "\n",
    "# Разбиение текста на предложения\n",
    "def split_into_sentences(text):\n",
    "    sentences = re.split(r'[.!?]', text)\n",
    "    return [sentence.strip() for sentence in sentences if sentence.strip()]\n",
    "\n",
    "# Основной процесс\n",
    "folder_path = Path(\"e:/cleaned_text\")\n",
    "texts = load_texts_from_folder(folder_path)\n",
    "\n",
    "# Разбиваем каждый текст на предложения\n",
    "sentences_list = [split_into_sentences(text) for text in texts]\n",
    "\n",
    "# Преобразуем список в одномерный список предложений\n",
    "all_sentences = [sentence for sublist in sentences_list for sentence in sublist]\n",
    "\n",
    "# Преобразуем в NumPy, если нужно\n",
    "text_data = np.array(all_sentences, dtype=object)\n",
    "\n",
    "# Функция для очистки биграмм и триграмм\n",
    "def clean_bigrams_trigrams(text):\n",
    "    # Убираем символы \"_\" в начале и конце, а также одиночные буквы\n",
    "    text = re.sub(r'^\\_+|\\_+$', '', text)  # Убираем символы \"_\" в начале и конце\n",
    "    text = re.sub(r'\\b\\w{1}\\b', '', text)  # Убираем одиночные буквы\n",
    "    text = re.sub(r'\\d+', '', text)  # Убираем цифры\n",
    "    return text.strip()\n",
    "\n",
    "# Функция для генерации биграмм и триграмм\n",
    "def generate_bigrams_trigrams(text_data):\n",
    "    # Генерация биграмм и триграмм\n",
    "    vectorizer = CountVectorizer(ngram_range=(1, 3), stop_words=russian_stopwords)\n",
    "    count_matrix = vectorizer.fit_transform(text_data)\n",
    "    \n",
    "    # Извлекаем биграммы и триграммы\n",
    "    ngram_features = vectorizer.get_feature_names_out()\n",
    "    \n",
    "    # Применяем очистку\n",
    "    cleaned_ngrams = [clean_bigrams_trigrams(ngram) for ngram in ngram_features]\n",
    "    \n",
    "    return cleaned_ngrams, count_matrix\n",
    "\n",
    "# Получаем очищенные биграммы и триграммы\n",
    "cleaned_bigrams_trigrams, count_matrix = generate_bigrams_trigrams(text_data)\n",
    "\n",
    "# Печать очищенных биграмм и триграмм\n",
    "# Печать первых 10 очищенных биграмм и триграмм\n",
    "print(cleaned_bigrams_trigrams[:40])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "2231666f-3d13-4a3c-8fb7-4e5ad838a44b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import re\n",
    "\n",
    "# Функция для очистки биграмм и триграмм\n",
    "def clean_bigrams_trigrams(text):\n",
    "    # Убираем символы \"_\" в начале и конце, а также одиночные буквы\n",
    "    text = re.sub(r'^\\_+|\\_+$', '', text)  # Убираем символы \"_\" в начале и конце\n",
    "    text = re.sub(r'\\b\\w{1}\\b', '', text)  # Убираем одиночные буквы\n",
    "    text = re.sub(r'\\d+', '', text)  # Убираем цифры\n",
    "    text = re.sub(r'[a-zA-Z]', '', text)  # Убираем латинские буквы (верхний и нижний регистр)\n",
    "    return text.strip()\n",
    "\n",
    "# Функция для генерации биграмм и триграмм\n",
    "def generate_bigrams_trigrams(text_data):\n",
    "    # Генерация биграмм и триграмм\n",
    "    vectorizer = CountVectorizer(ngram_range=(1, 3), stop_words='english')\n",
    "    count_matrix = vectorizer.fit_transform(text_data)\n",
    "    \n",
    "    # Извлекаем биграммы и триграммы\n",
    "    ngram_features = vectorizer.get_feature_names_out()\n",
    "    \n",
    "    # Применяем очистку\n",
    "    cleaned_ngrams = [clean_bigrams_trigrams(ngram) for ngram in ngram_features]\n",
    "    \n",
    "    return cleaned_ngrams, count_matrix\n",
    "\n",
    "# Получаем очищенные биграммы и триграммы\n",
    "cleaned_bigrams_trigrams, count_matrix = generate_bigrams_trigrams(text_data)\n",
    "\n",
    "# Преобразуем разреженную матрицу в DataFrame для частот\n",
    "ngram_frequencies = np.array(count_matrix.sum(axis=0)).flatten()\n",
    "\n",
    "# Разделяем на биграммы и триграммы\n",
    "bigrams = [ngram for ngram in cleaned_bigrams_trigrams if len(ngram.split()) == 2]\n",
    "bigrams_frequencies = ngram_frequencies[[i for i, ngram in enumerate(cleaned_bigrams_trigrams) if len(ngram.split()) == 2]]\n",
    "\n",
    "trigrams = [ngram for ngram in cleaned_bigrams_trigrams if len(ngram.split()) == 3]\n",
    "trigrams_frequencies = ngram_frequencies[[i for i, ngram in enumerate(cleaned_bigrams_trigrams) if len(ngram.split()) == 3]]\n",
    "\n",
    "# Создаем pandas таблицы для биграмм и триграмм\n",
    "bigrams_df = pd.DataFrame({'Bigram': bigrams, 'Frequency': bigrams_frequencies})\n",
    "trigrams_df = pd.DataFrame({'Trigram': trigrams, 'Frequency': trigrams_frequencies})\n",
    "\n",
    "# Выводим первые 40 биграмм и триграмм\n",
    "# print(\"Bigrams:\")\n",
    "# print(bigrams_df.head(40))\n",
    "\n",
    "# print(\"\\nTrigrams:\")\n",
    "# print(trigrams_df.head(40))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "f0c9b26f-3b25-4be8-b01f-c33c4737d98e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bigrams:\n",
      "                          Bigram  Frequency\n",
      "0               иностранных слов          2\n",
      "1                  рублей дохода          1\n",
      "2                 экземпляров во          1\n",
      "3                   августа буду          1\n",
      "4                 августа выехал          1\n",
      "5                    без подписи          1\n",
      "6              библейского поэта          1\n",
      "7   благонамеренная бестактность          1\n",
      "8                       боже мой          2\n",
      "9                        вот это          2\n",
      "10                   всем шильям          1\n",
      "11          вызвал сочувственную          1\n",
      "12              выражением франц          2\n",
      "13                 галманы олухи          1\n",
      "14               гимназия велась          1\n",
      "15                    го августа          2\n",
      "16                     го апреля          1\n",
      "17                    го генваря          1\n",
      "18                        го мая          1\n",
      "19                    декабря на          1\n",
      "20                  желаем знать          1\n",
      "21                   житный двор          1\n",
      "22                замысле романа          1\n",
      "23                 июля дошедшее          1\n",
      "24                  июля приедут          1\n",
      "25                карда изгородь          1\n",
      "26                километрах юго          1\n",
      "27                кравчий боярин          1\n",
      "28                    лет мелочи          1\n",
      "29                     лет право          1\n",
      "30                      лет тому          1\n",
      "31                 летия русской          1\n",
      "32                 мая останется          1\n",
      "33                 мая присылкою          1\n",
      "34                мая состоялись          1\n",
      "35              мельничиха марли          1\n",
      "36                 молчанов один          1\n",
      "37                   надлежит не          1\n",
      "38                    не человек          1\n",
      "39                      но потом          1\n"
     ]
    }
   ],
   "source": [
    "# Выводим первые 40 биграмм и триграмм\n",
    "print(\"Bigrams:\")\n",
    "print(bigrams_df.head(40))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "f40114b0-03bb-4c7a-8465-16217f1d75b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Trigrams:\n",
      "                            Trigram  Frequency\n",
      "0   з способствовал археологическим          1\n",
      "1                 год был министром          1\n",
      "2                   де бараль сыном          1\n",
      "3                     т говорит как          1\n",
      "4         торично флологчного вддлу          1\n",
      "5                     з розшукв про          1\n",
      "6    сторично філологічного відділу          1\n",
      "7                   е бертольди для          1\n",
      "8               аа кормили отвечала          1\n",
      "9              ддв великих прауноки          1\n",
      "10             видом нашим мартыном          1\n",
      "11                   і чоловіки мав          1\n",
      "12                     і языки умів          1\n",
      "13                аиерей может быть          1\n",
      "14               аарон митре жезлом          1\n",
      "15                    аарон один из          1\n",
      "16           аарона воткнутый землю          1\n",
      "17              аарона дафан авирон          1\n",
      "18                   аарония во всю          1\n",
      "19             ааронов отцу захарии          1\n",
      "20    ааронов первосвященника брата          1\n",
      "21            ааронов расцвел вышел          1\n",
      "22           ааронов расцвел пустил          1\n",
      "23                ааронове вот жезл          1\n",
      "24      аафанасьевич двуличный пред          1\n",
      "25            абадонна читал губера          1\n",
      "26                абажуре лампы так          1\n",
      "27             абажуром свеча перед          1\n",
      "28            абажуром читала вслух          1\n",
      "29        абажурчик усевшись уголке          1\n",
      "30            абандона коротала дни          1\n",
      "31            абандона покинутая от          1\n",
      "32               абассидов но этого          1\n",
      "33     абассиды правильнее аббасиды          1\n",
      "34           аббадона взоры потупив          1\n",
      "35              аббат егриньи герои          1\n",
      "36                аббат егриньи как          1\n",
      "37               аббат лакордер жан          1\n",
      "38                     аббат на том          1\n",
      "39              аббата лакордера об          1\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nTrigrams:\")\n",
    "print(trigrams_df.head(40))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "679934fd-2c0e-46fe-b803-87cc5f2fad50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                               Trigram  Frequency\n",
      "941064        прочтения известного нам          1\n",
      "822240             писания но говорить          1\n",
      "1035255              сию самую несытую          1\n",
      "360210                    захочу но не          1\n",
      "776170         от древних владетельных          1\n",
      "46032                  було десь давно          1\n",
      "551527            моей доверенности по          1\n",
      "837035                    по форме под          1\n",
      "535316                   мира где люди          1\n",
      "808281           пачками руках кабинет          1\n",
      "1216687          учился отлично учился          1\n",
      "703204            новом этапе развития          1\n",
      "1009086               своим делом было          1\n",
      "846786                   поди тебе эту          1\n",
      "956662               разговоре это для          1\n",
      "838740              повелеть чтобы при          1\n",
      "4672       альбоме впечатлений заметок          1\n",
      "747033        он повернулся компрессом          1\n",
      "712490            об этико философских          1\n",
      "1246292           человек ней домертва          1\n",
      "449975         когда холод пересиливал          1\n",
      "260013          домика сразу своротили          1\n",
      "271167      дружбы требующей равенства          1\n",
      "681508                нигде нет такого          1\n",
      "1071330  создаться настоящий искренний          1\n",
      "161125                 выдать ее замуж          2\n",
      "66640                    было тебе ним          1\n",
      "391400           изограф севастьян был          1\n",
      "852318       поехали митрополиту лавру          1\n",
      "1196576              уезжаю из франции          1\n",
      "1117922           так мешковата ничего          1\n",
      "137626                     все стал на          1\n",
      "69136           быть немножко страдает          1\n",
      "292823                  ее куда сунуть          1\n",
      "467406                которые могли бы          5\n",
      "220230                      дар сам по          1\n",
      "1326000              ямке под заметным          1\n",
      "26419          благоуханные розы тогда          1\n",
      "880741                после после того          1\n",
      "171422                   где был кабак          1\n"
     ]
    }
   ],
   "source": [
    "print(trigrams_df.sample(40))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "ba86c61f-8108-4e06-b9ea-614434b6f0c1",
   "metadata": {},
   "outputs": [
    {
     "ename": "InvalidParameterError",
     "evalue": "The 'stop_words' parameter of CountVectorizer must be a str among {'english'}, an instance of 'list' or None. Got 'russian' instead.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInvalidParameterError\u001b[0m                     Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[66], line 55\u001b[0m\n\u001b[0;32m     53\u001b[0m \u001b[38;5;66;03m# Пример использования\u001b[39;00m\n\u001b[0;32m     54\u001b[0m target_word \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mлюбовь\u001b[39m\u001b[38;5;124m\"\u001b[39m  \u001b[38;5;66;03m# Слово для поиска похожих слов\u001b[39;00m\n\u001b[1;32m---> 55\u001b[0m \u001b[43mfind_similar_words\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_word\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[66], line 24\u001b[0m, in \u001b[0;36mfind_similar_words\u001b[1;34m(texts, target_word)\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfind_similar_words\u001b[39m(texts, target_word):\n\u001b[0;32m     23\u001b[0m     \u001b[38;5;66;03m# Векторизация текста\u001b[39;00m\n\u001b[1;32m---> 24\u001b[0m     X, vectorizer \u001b[38;5;241m=\u001b[39m \u001b[43mvectorize_texts\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     26\u001b[0m     \u001b[38;5;66;03m# Получаем индексы всех слов\u001b[39;00m\n\u001b[0;32m     27\u001b[0m     feature_names \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(vectorizer\u001b[38;5;241m.\u001b[39mget_feature_names_out())\n",
      "Cell \u001b[1;32mIn[66], line 18\u001b[0m, in \u001b[0;36mvectorize_texts\u001b[1;34m(texts)\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mvectorize_texts\u001b[39m(texts):\n\u001b[0;32m     17\u001b[0m     vectorizer \u001b[38;5;241m=\u001b[39m CountVectorizer(stop_words\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrussian\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m---> 18\u001b[0m     X \u001b[38;5;241m=\u001b[39m \u001b[43mvectorizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     19\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m X, vectorizer\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\base.py:1466\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1461\u001b[0m partial_fit_and_fitted \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m   1462\u001b[0m     fit_method\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpartial_fit\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m _is_fitted(estimator)\n\u001b[0;32m   1463\u001b[0m )\n\u001b[0;32m   1465\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m global_skip_validation \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m partial_fit_and_fitted:\n\u001b[1;32m-> 1466\u001b[0m     \u001b[43mestimator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_params\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1468\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1469\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1470\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1471\u001b[0m     )\n\u001b[0;32m   1472\u001b[0m ):\n\u001b[0;32m   1473\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\base.py:666\u001b[0m, in \u001b[0;36mBaseEstimator._validate_params\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    658\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_validate_params\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    659\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Validate types and values of constructor parameters\u001b[39;00m\n\u001b[0;32m    660\u001b[0m \n\u001b[0;32m    661\u001b[0m \u001b[38;5;124;03m    The expected type and values must be defined in the `_parameter_constraints`\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    664\u001b[0m \u001b[38;5;124;03m    accepted constraints.\u001b[39;00m\n\u001b[0;32m    665\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 666\u001b[0m     \u001b[43mvalidate_parameter_constraints\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    667\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_parameter_constraints\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    668\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_params\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdeep\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    669\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcaller_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;18;43m__class__\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;18;43m__name__\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    670\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py:95\u001b[0m, in \u001b[0;36mvalidate_parameter_constraints\u001b[1;34m(parameter_constraints, params, caller_name)\u001b[0m\n\u001b[0;32m     89\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     90\u001b[0m     constraints_str \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m     91\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin([\u001b[38;5;28mstr\u001b[39m(c)\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mc\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39mconstraints[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]])\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m or\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     92\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconstraints[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     93\u001b[0m     )\n\u001b[1;32m---> 95\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m InvalidParameterError(\n\u001b[0;32m     96\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mparam_name\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m parameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcaller_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     97\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconstraints_str\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. Got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mparam_val\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     98\u001b[0m )\n",
      "\u001b[1;31mInvalidParameterError\u001b[0m: The 'stop_words' parameter of CountVectorizer must be a str among {'english'}, an instance of 'list' or None. Got 'russian' instead."
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bc1bd0d-5898-4e2c-b268-c8c94ac62fed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Создаем DataFrame для биграмм и триграмм\n",
    "bigram_df = pd.DataFrame({\n",
    "    'Bigram/Trigram': cleaned_bigrams_trigrams,\n",
    "    'Frequency': count_matrix.sum(axis=0).A1\n",
    "})\n",
    "\n",
    "# Сохраняем в CSV файл\n",
    "bigram_df.to_csv(\"bigrams_trigrams.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "711ba495-cd7a-44a3-b26e-4086d8deaf1a",
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 2.14 TiB for an array with shape (149142, 1973550) and data type int64",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[69], line 6\u001b[0m\n\u001b[0;32m      3\u001b[0m count_matrix \u001b[38;5;241m=\u001b[39m count\u001b[38;5;241m.\u001b[39mfit_transform(text_data)\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# Преобразуем в DataFrame для удобства\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m count_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(\u001b[43mcount_matrix\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtoarray\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m, columns\u001b[38;5;241m=\u001b[39mcount\u001b[38;5;241m.\u001b[39mget_feature_names_out())\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# Получаем частоты для каждого термина (нормализуем, если нужно)\u001b[39;00m\n\u001b[0;32m      9\u001b[0m frequency \u001b[38;5;241m=\u001b[39m count_df\u001b[38;5;241m.\u001b[39msum(axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\scipy\\sparse\\_compressed.py:1106\u001b[0m, in \u001b[0;36m_cs_matrix.toarray\u001b[1;34m(self, order, out)\u001b[0m\n\u001b[0;32m   1104\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m out \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m order \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1105\u001b[0m     order \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_swap(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcf\u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m-> 1106\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_process_toarray_args\u001b[49m\u001b[43m(\u001b[49m\u001b[43morder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1107\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (out\u001b[38;5;241m.\u001b[39mflags\u001b[38;5;241m.\u001b[39mc_contiguous \u001b[38;5;129;01mor\u001b[39;00m out\u001b[38;5;241m.\u001b[39mflags\u001b[38;5;241m.\u001b[39mf_contiguous):\n\u001b[0;32m   1108\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mOutput array must be C or F contiguous\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\scipy\\sparse\\_base.py:1327\u001b[0m, in \u001b[0;36m_spbase._process_toarray_args\u001b[1;34m(self, order, out)\u001b[0m\n\u001b[0;32m   1325\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m out\n\u001b[0;32m   1326\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1327\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39mzeros(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mshape, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdtype, order\u001b[38;5;241m=\u001b[39morder)\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 2.14 TiB for an array with shape (149142, 1973550) and data type int64"
     ]
    }
   ],
   "source": [
    "# Получаем частоту для каждого термина (если уже используете CountVectorizer)\n",
    "count = CountVectorizer(ngram_range=(1, 3), stop_words=russian_stopwords)\n",
    "count_matrix = count.fit_transform(text_data)\n",
    "\n",
    "# Преобразуем в DataFrame для удобства\n",
    "count_df = pd.DataFrame(count_matrix.toarray(), columns=count.get_feature_names_out())\n",
    "\n",
    "# Получаем частоты для каждого термина (нормализуем, если нужно)\n",
    "frequency = count_df.sum(axis=0)\n",
    "\n",
    "# Создаём DataFrame с биграммами и частотой\n",
    "bigram_df = pd.DataFrame({\n",
    "    'Bigram': frequency.index,\n",
    "    'Frequency': frequency.values\n",
    "})\n",
    "\n",
    "# Для нормализации, например, делим на количество слов в документе:\n",
    "# total_words = sum(frequency)\n",
    "# bigram_df['Normalized Frequency'] = bigram_df['Frequency'] / total_words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "1191d686-a038-464c-a83f-3552b29ebd2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       Bigram  Frequency\n",
      "0  потому что       2210\n",
      "1      что он       1749\n",
      "2     что это       1544\n",
      "3     все это       1386\n",
      "4  может быть       1268\n",
      "5   ничего не       1240\n",
      "6      что же       1210\n",
      "7      то что       1131\n",
      "8     не было       1113\n",
      "9     то есть        963\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from pathlib import Path\n",
    "\n",
    "# Функция для загрузки текстов из папки\n",
    "def load_texts_from_folder(folder_path):\n",
    "    texts = []\n",
    "    for file_path in Path(folder_path).glob(\"*.txt\"):  # Чтение всех текстов с расширением .txt\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "            texts.append(file.read())\n",
    "    return texts\n",
    "\n",
    "# Указываем путь к папке с текстами Н. С. Лескова\n",
    "folder_path = \"e:/cleaned_text\"  # Путь к папке, где находятся файлы\n",
    "\n",
    "# Загружаем все тексты\n",
    "texts = load_texts_from_folder(folder_path)\n",
    "\n",
    "# Преобразуем в CountVectorizer (извлечение биграмм)\n",
    "vectorizer = CountVectorizer(ngram_range=(2, 2), stop_words=None)  # Для биграмм\n",
    "X = vectorizer.fit_transform(texts)\n",
    "\n",
    "# Получаем все биграммы\n",
    "ngram_features = vectorizer.get_feature_names_out()\n",
    "\n",
    "# Подсчитываем частоту биграмм\n",
    "frequency = X.sum(axis=0).A1  # Преобразуем в одномерный массив\n",
    "\n",
    "# Преобразуем в pandas Series\n",
    "frequency_series = pd.Series(frequency, index=ngram_features)\n",
    "\n",
    "# Сортируем по частоте и ограничиваем количество\n",
    "top_n = 10  # Ограничиваем до 10 самых частых биграмм\n",
    "top_bigrams = frequency_series.sort_values(ascending=False).head(top_n)\n",
    "\n",
    "# Создаем DataFrame с ограниченным количеством биграмм\n",
    "bigram_df = pd.DataFrame({\n",
    "    'Bigram': top_bigrams.index,\n",
    "    'Frequency': top_bigrams.values\n",
    "})\n",
    "\n",
    "# Печать результата\n",
    "print(bigram_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "56f9ea46-dfda-4b58-8f03-f2d7af085405",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\alexewd\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 наиболее похожих слов к слову 'любовь':\n",
      "любовь\n",
      "молодых\n",
      "любви\n",
      "молодой\n",
      "жена\n",
      "старики\n",
      "устали\n",
      "платить\n",
      "движением\n",
      "никак\n",
      "временем\n",
      "ехать\n",
      "смотреть\n",
      "ногу\n",
      "живет\n",
      "длинное\n",
      "красивый\n",
      "раза\n",
      "пальцы\n",
      "денег\n",
      "рукою\n",
      "нервы\n",
      "свободно\n",
      "своими\n",
      "успокоился\n",
      "прочие\n",
      "поле\n",
      "расположение\n",
      "свои\n",
      "никого\n",
      "зорко\n",
      "дорогой\n",
      "дороге\n",
      "старика\n",
      "радости\n",
      "охота\n",
      "знает\n",
      "забыл\n",
      "жене\n",
      "столом\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from pathlib import Path\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Загрузка стоп-слов для русского языка\n",
    "nltk.download('stopwords')\n",
    "russian_stopwords = stopwords.words('russian')\n",
    "\n",
    "# Загрузка текстов из папки\n",
    "def load_texts_from_folder(folder_path):\n",
    "    texts = []\n",
    "    for file_path in Path(folder_path).glob(\"*.txt\"):\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            texts.append(f.read())\n",
    "    return texts\n",
    "\n",
    "# Преобразуем тексты в матрицу термов\n",
    "def vectorize_texts(texts):\n",
    "    vectorizer = CountVectorizer(stop_words=russian_stopwords)  # Передаем список стоп-слов\n",
    "    X = vectorizer.fit_transform(texts)\n",
    "    return X, vectorizer\n",
    "\n",
    "# Функция для поиска похожих слов\n",
    "def find_similar_words(texts, target_word):\n",
    "    # Векторизация текста\n",
    "    X, vectorizer = vectorize_texts(texts)\n",
    "\n",
    "    # Получаем индексы всех слов\n",
    "    feature_names = np.array(vectorizer.get_feature_names_out())\n",
    "\n",
    "    # Проверяем, есть ли целевое слово в словаре\n",
    "    if target_word not in feature_names:\n",
    "        print(f\"Слово '{target_word}' не найдено в тексте.\")\n",
    "        return\n",
    "\n",
    "    # Получаем вектор для целевого слова\n",
    "    target_idx = np.where(feature_names == target_word)[0][0]\n",
    "    target_vector = X[:, target_idx].toarray().flatten()\n",
    "\n",
    "    # Считаем косинусное сходство между словом и всеми остальными словами\n",
    "    similarities = cosine_similarity(X.T, target_vector.reshape(1, -1))\n",
    "\n",
    "    # Получаем индексы наиболее похожих слов\n",
    "    similar_idx = similarities.flatten().argsort()[::-1]\n",
    "\n",
    "    # Печатаем 10 наиболее похожих слов\n",
    "    print(f\"10 наиболее похожих слов к слову '{target_word}':\")\n",
    "    for idx in similar_idx[:40]:\n",
    "        print(feature_names[idx])\n",
    "\n",
    "# Загрузка всех текстов\n",
    "folder_path = Path(\"e:/cleaned_text\")  # Указываем путь к папке с текстами\n",
    "texts = load_texts_from_folder(folder_path)\n",
    "\n",
    "# Пример использования\n",
    "target_word = \"любовь\"  # Слово для поиска похожих слов\n",
    "find_similar_words(texts, target_word)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2a1db19f-8969-4213-b6bd-06c0b54492b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Монограммы:\n",
      "   Monogram      TF-IDF     Frequency\n",
      "0   говорит  665.540584  66554.058448\n",
      "1     такое  554.599412  55459.941211\n",
      "2    говорю  540.897516  54089.751551\n",
      "3      дело  457.104448  45710.444800\n",
      "4      знаю  452.300844  45230.084386\n",
      "5      тебе  449.538056  44953.805623\n",
      "6     время  392.089052  39208.905215\n",
      "7   отвечал  377.465160  37746.515957\n",
      "8    сказал  374.216788  37421.678850\n",
      "9   человек  357.336025  35733.602496\n",
      "10       см  335.696229  33569.622867\n",
      "11   отчего  334.583309  33458.330913\n",
      "12  лескова  329.000122  32900.012187\n",
      "13     могу  300.047851  30004.785134\n",
      "14      нам  290.776728  29077.672773\n",
      "15     люди  290.639733  29063.973281\n",
      "16  спросил  290.082771  29008.277101\n",
      "17  который  283.800969  28380.096932\n",
      "18   делать  273.530325  27353.032521\n",
      "19     всем  273.195163  27319.516322\n",
      "\n",
      "Биграммы и Триграммы:\n",
      "                        Ngram      TF-IDF     Frequency\n",
      "0         публикуется впервые  229.395895  22939.589518\n",
      "1        печатается автографу  124.552079  12455.207882\n",
      "2                  самом деле   85.635199   8563.519884\n",
      "3       жизнь николая лескова   75.937683   7593.768255\n",
      "4               жизнь николая   75.937683   7593.768255\n",
      "5             николая лескова   75.937683   7593.768255\n",
      "6              автографу ирли   72.812541   7281.254137\n",
      "7   печатается автографу ирли   72.342836   7234.283550\n",
      "8        впервые опубликовано   64.386294   6438.629368\n",
      "9          собрание сочинений   60.212877   6021.287704\n",
      "10          примечание письму   48.219905   4821.990488\n",
      "11          лизавета егоровна   45.690499   4569.049903\n",
      "12               крайней мере   40.005751   4000.575147\n",
      "13                    сих пор   38.290056   3829.005619\n",
      "14          шестидесятые годы   36.259000   3625.900009\n",
      "15               имеется виду   35.759862   3575.986180\n",
      "16           катерина львовна   35.440780   3544.078020\n",
      "17           домна платоновна   34.996347   3499.634670\n",
      "18                    тех пор   32.590781   3259.078091\n",
      "19                  речь идет   31.199367   3119.936728\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Используем TfidfVectorizer для монограмм\n",
    "tfidf_monograms = TfidfVectorizer(ngram_range=(1, 1), stop_words=russian_stopwords)\n",
    "monogram_feature_matrix = tfidf_monograms.fit_transform(text_data)\n",
    "\n",
    "# Получаем монограммы и их значения TF-IDF\n",
    "monogram_feature_names = tfidf_monograms.get_feature_names_out()\n",
    "monogram_tfidf_scores = monogram_feature_matrix.sum(axis=0).A1\n",
    "\n",
    "# Создаем список из монограмм и их значений\n",
    "monogram_features_with_scores = list(zip(monogram_feature_names, monogram_tfidf_scores))\n",
    "\n",
    "# Создаем DataFrame для монограмм\n",
    "monogram_df = pd.DataFrame(monogram_features_with_scores, columns=[\"Monogram\", \"TF-IDF\"])\n",
    "monogram_df[\"Frequency\"] = monogram_df[\"TF-IDF\"].apply(lambda x: x * 100)  # Примерная частота на основе TF-IDF\n",
    "monogram_df = monogram_df.sort_values(by=\"TF-IDF\", ascending=False).reset_index(drop=True)\n",
    "\n",
    "# Используем TfidfVectorizer для биграмм и триграмм\n",
    "tfidf_ngrams = TfidfVectorizer(ngram_range=(2, 3), stop_words=russian_stopwords)\n",
    "ngram_feature_matrix = tfidf_ngrams.fit_transform(text_data)\n",
    "\n",
    "# Получаем биграммы и триграммы с их значениями TF-IDF\n",
    "ngram_feature_names = tfidf_ngrams.get_feature_names_out()\n",
    "ngram_tfidf_scores = ngram_feature_matrix.sum(axis=0).A1\n",
    "\n",
    "# Создаем список из биграмм и триграмм и их значений\n",
    "ngram_features_with_scores = list(zip(ngram_feature_names, ngram_tfidf_scores))\n",
    "\n",
    "# Фильтруем биграммы и триграммы (убираем цифры)\n",
    "filtered_ngram_features = [\n",
    "    (feature, score) for feature, score in ngram_features_with_scores if not re.search(r\"\\d\", feature)\n",
    "]\n",
    "\n",
    "# Создаем DataFrame для биграмм и триграмм\n",
    "ngram_df = pd.DataFrame(filtered_ngram_features, columns=[\"Ngram\", \"TF-IDF\"])\n",
    "ngram_df[\"Frequency\"] = ngram_df[\"TF-IDF\"].apply(lambda x: x * 100)  # Примерная частота\n",
    "ngram_df = ngram_df.sort_values(by=\"TF-IDF\", ascending=False).reset_index(drop=True)\n",
    "\n",
    "# Выводим таблицы\n",
    "print(\"Монограммы:\")\n",
    "print(monogram_df.head(20))  # Топ-10 монограмм\n",
    "\n",
    "print(\"\\nБиграммы и Триграммы:\")\n",
    "print(ngram_df.head(20))  # Топ-10 биграмм и триграмм\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1cf7d49e-35f7-4474-925b-3b392d240ebb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Монограммы:\n",
      "   Monogram      TF-IDF     Frequency\n",
      "0   говорит  665.540584  66554.058448\n",
      "1     такое  554.599412  55459.941211\n",
      "2    говорю  540.897516  54089.751551\n",
      "3      дело  457.104448  45710.444800\n",
      "4      знаю  452.300844  45230.084386\n",
      "5      тебе  449.538056  44953.805623\n",
      "6     время  392.089052  39208.905215\n",
      "7   отвечал  377.465160  37746.515957\n",
      "8    сказал  374.216788  37421.678850\n",
      "9   человек  357.336025  35733.602496\n",
      "10       см  335.696229  33569.622867\n",
      "11   отчего  334.583309  33458.330913\n",
      "12  лескова  329.000122  32900.012187\n",
      "13     могу  300.047851  30004.785134\n",
      "14      нам  290.776728  29077.672773\n",
      "15     люди  290.639733  29063.973281\n",
      "16  спросил  290.082771  29008.277101\n",
      "17  который  283.800969  28380.096932\n",
      "18   делать  273.530325  27353.032521\n",
      "19     всем  273.195163  27319.516322\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Используем TfidfVectorizer для монограмм\n",
    "tfidf_monograms = TfidfVectorizer(ngram_range=(1, 1), stop_words=russian_stopwords)\n",
    "monogram_feature_matrix = tfidf_monograms.fit_transform(text_data)\n",
    "\n",
    "# Получаем монограммы и их значения TF-IDF\n",
    "monogram_feature_names = tfidf_monograms.get_feature_names_out()\n",
    "monogram_tfidf_scores = monogram_feature_matrix.sum(axis=0).A1\n",
    "\n",
    "# Создаем список из монограмм и их значений\n",
    "monogram_features_with_scores = list(zip(monogram_feature_names, monogram_tfidf_scores))\n",
    "\n",
    "# Создаем DataFrame для монограмм\n",
    "monogram_df = pd.DataFrame(monogram_features_with_scores, columns=[\"Monogram\", \"TF-IDF\"])\n",
    "monogram_df[\"Frequency\"] = monogram_df[\"TF-IDF\"].apply(lambda x: x * 100)  # Примерная частота на основе TF-IDF\n",
    "monogram_df = monogram_df.sort_values(by=\"TF-IDF\", ascending=False).reset_index(drop=True)\n",
    "\n",
    "# Используем TfidfVectorizer для биграмм и триграмм\n",
    "tfidf_ngrams = TfidfVectorizer(ngram_range=(2, 3), stop_words=russian_stopwords)\n",
    "ngram_feature_matrix = tfidf_ngrams.fit_transform(text_data)\n",
    "\n",
    "# Получаем биграммы и триграммы с их значениями TF-IDF\n",
    "ngram_feature_names = tfidf_ngrams.get_feature_names_out()\n",
    "ngram_tfidf_scores = ngram_feature_matrix.sum(axis=0).A1\n",
    "\n",
    "# Создаем список из биграмм и триграмм и их значений\n",
    "ngram_features_with_scores = list(zip(ngram_feature_names, ngram_tfidf_scores))\n",
    "\n",
    "# Фильтруем биграммы и триграммы (убираем цифры)\n",
    "filtered_ngram_features = [\n",
    "    (feature, score) for feature, score in ngram_features_with_scores if not re.search(r\"\\d\", feature)\n",
    "]\n",
    "\n",
    "# Разделяем на биграммы и триграммы\n",
    "bigram_features = [(feature, score) for feature, score in filtered_ngram_features if len(feature.split()) == 2]\n",
    "trigram_features = [(feature, score) for feature, score in filtered_ngram_features if len(feature.split()) == 3]\n",
    "\n",
    "# Создаем DataFrame для биграмм\n",
    "bigram_df = pd.DataFrame(bigram_features, columns=[\"Bigram\", \"TF-IDF\"])\n",
    "bigram_df[\"Frequency\"] = bigram_df[\"TF-IDF\"].apply(lambda x: x * 100)  # Примерная частота\n",
    "bigram_df = bigram_df.sort_values(by=\"TF-IDF\", ascending=False).reset_index(drop=True)\n",
    "\n",
    "# Создаем DataFrame для триграмм\n",
    "trigram_df = pd.DataFrame(trigram_features, columns=[\"Trigram\", \"TF-IDF\"])\n",
    "trigram_df[\"Frequency\"] = trigram_df[\"TF-IDF\"].apply(lambda x: x * 100)  # Примерная частота\n",
    "trigram_df = trigram_df.sort_values(by=\"TF-IDF\", ascending=False).reset_index(drop=True)\n",
    "\n",
    "# Выводим таблицы\n",
    "print(\"Монограммы:\")\n",
    "print(monogram_df.head(20))  # Топ-10 монограмм\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c67bc0b0-8c79-4efe-ba0e-aa4a60349905",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Биграммы:\n",
      "                   Bigram      TF-IDF     Frequency\n",
      "0     публикуется впервые  229.395895  22939.589518\n",
      "1    печатается автографу  124.552079  12455.207882\n",
      "2              самом деле   85.635199   8563.519884\n",
      "3         николая лескова   75.937683   7593.768255\n",
      "4           жизнь николая   75.937683   7593.768255\n",
      "5          автографу ирли   72.812541   7281.254137\n",
      "6    впервые опубликовано   64.386294   6438.629368\n",
      "7      собрание сочинений   60.212877   6021.287704\n",
      "8       примечание письму   48.219905   4821.990488\n",
      "9       лизавета егоровна   45.690499   4569.049903\n",
      "10           крайней мере   40.005751   4000.575147\n",
      "11                сих пор   38.290056   3829.005619\n",
      "12      шестидесятые годы   36.259000   3625.900009\n",
      "13           имеется виду   35.759862   3575.986180\n",
      "14       катерина львовна   35.440780   3544.078020\n",
      "15       домна платоновна   34.996347   3499.634670\n",
      "16                тех пор   32.590781   3259.078091\n",
      "17              речь идет   31.199367   3119.936728\n",
      "18  опубликовано сборнике   31.002628   3100.262757\n",
      "19             друг друга   29.966427   2996.642707\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nБиграммы:\")\n",
    "print(bigram_df.head(20))  # Топ-10 биграмм"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "088ae47d-64f0-4d6d-bb58-71d0c0ed48c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Триграммы:\n",
      "                               Trigram     TF-IDF    Frequency\n",
      "0                жизнь николая лескова  75.937683  7593.768255\n",
      "1            печатается автографу ирли  72.342836  7234.283550\n",
      "2        впервые опубликовано сборнике  31.002628  3100.262757\n",
      "3           впервые опубликовано книге  28.821684  2882.168376\n",
      "4           печатается автографу цгали  28.175346  2817.534641\n",
      "5           сборнике шестидесятые годы  23.876243  2387.624265\n",
      "6             печатается автографу гпб  22.968583  2296.858301\n",
      "7   опубликовано сборнике шестидесятые  22.854501  2285.450140\n",
      "8            полное собрание сочинений  19.419614  1941.961394\n",
      "9                   ваш покорный слуга  13.548370  1354.836955\n",
      "10         уважаемый алексей сергеевич   9.812641   981.264117\n",
      "11             уважаемый петр карлович   9.727565   972.756467\n",
      "12    печатается автографу толстовский   9.687288   968.728840\n",
      "13         автографу толстовский музей   9.687288   968.728840\n",
      "14            сборнике письма толстого   9.386241   938.624114\n",
      "15        опубликовано сборнике письма   9.386241   938.624114\n",
      "16                            ха ха ха   9.010792   901.079206\n",
      "17             печатается автографу лб   8.933775   893.377528\n",
      "18            говорит домна платоновна   8.202548   820.254813\n",
      "19        впервые опубликовано журнале   7.812947   781.294670\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nТриграммы:\")\n",
    "print(trigram_df.head(20))  # Топ-10 триграмм"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86a244c5-5e70-4f73-a098-1677065cfd7b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2c35072c-ba5c-438c-a293-1d5bbc2c1ab6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Топ-10 значимых слов и биграмм по TF-IDF:\n",
      "это: 0.6229\n",
      "очень: 0.2434\n",
      "лесков: 0.1868\n",
      "стр: 0.1290\n",
      "лескова: 0.1209\n",
      "печатается: 0.1061\n",
      "письмо: 0.1039\n",
      "петербург: 0.1018\n",
      "впервые: 0.0946\n",
      "время: 0.0894\n",
      "автографу: 0.0857\n",
      "печатается автографу: 0.0841\n",
      "который: 0.0801\n",
      "года: 0.0739\n",
      "толстого: 0.0705\n",
      "кажется: 0.0690\n",
      "дело: 0.0680\n",
      "письма: 0.0674\n",
      "нем: 0.0628\n",
      "см: 0.0628\n",
      "которые: 0.0619\n",
      "могу: 0.0612\n",
      "знаю: 0.0551\n",
      "публикуется: 0.0532\n",
      "публикуется впервые: 0.0532\n",
      "человек: 0.0523\n",
      "рассказ: 0.0510\n",
      "люди: 0.0507\n",
      "вами: 0.0492\n",
      "ваше: 0.0492\n",
      "которого: 0.0467\n",
      "этим: 0.0467\n",
      "людей: 0.0464\n",
      "которых: 0.0448\n",
      "статьи: 0.0448\n",
      "ваш: 0.0436\n",
      "времени: 0.0436\n",
      "жизни: 0.0436\n",
      "ирли: 0.0433\n",
      "публикация: 0.0433\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Выводим топ-10 самых значимых слов и биграмм\n",
    "print(\"Топ-10 значимых слов и биграмм по TF-IDF:\")\n",
    "for feature, score in filtered_features[:40]:\n",
    "    print(f\"{feature}: {score:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9750b1c8-a1e0-409e-bb24-7b5248def312",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Форма bag_of_words: (1, 38043)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Выводим примерный результат\n",
    "# print(\"Пример предложений:\", text_data[:5])\n",
    "print(\"Форма bag_of_words:\", bag_of_words.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf769ee3-b771-4c90-83ba-40f2ed821754",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
