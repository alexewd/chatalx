{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Анализ авторства текстов\n",
    "Этот блокнот анализирует два текстовых файла (`author_1.txt` и `author_2.txt`) и оценивает вероятность того, что они написаны одним автором."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Установка зависимостей\n",
    "Установите библиотеки (выполните в терминале или через !):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install nltk fuzzywuzzy python-Levenshtein matplotlib spacy scikit-learn pandas\n",
    "# !python -m spacy download ru_core_news_sm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Импорт библиотек"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\alexewd\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\alexewd\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.util import ngrams\n",
    "from collections import Counter\n",
    "import re\n",
    "from fuzzywuzzy import fuzz\n",
    "import spacy\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "\n",
    "# Загрузка данных NLTK\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Загрузка модели spacy для русского языка\n",
    "nlp = spacy.load('ru_core_news_sm')\n",
    "nlp = spacy.load('ru_core_news_sm')\n",
    "nlp.max_length = 2000000  # Достаточно для 1,446,486 символов\n",
    "# Установка отображения графиков\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Функции для анализа"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87e04276",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Чтение файла\n",
    "def read_file(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        return file.read()\n",
    "\n",
    "# Очистка текста\n",
    "def clean_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^а-яё\\s]', '', text)  # Оставляем только русские буквы и пробелы\n",
    "    return text\n",
    "\n",
    "# Частотность слов\n",
    "def word_frequency(text):\n",
    "    words = word_tokenize(clean_text(text))\n",
    "    stop_words = set(stopwords.words('russian'))\n",
    "    words = [word for word in words if word and word not in stop_words]  # Убираем пустые строки\n",
    "    if not words:  # Отладка\n",
    "        print(\"Ошибка: список слов пустой после токенизации\")\n",
    "        return Counter()\n",
    "    return Counter(words)\n",
    "\n",
    "# Биграммы\n",
    "def ngram_frequency(text, n=2):\n",
    "    words = word_tokenize(clean_text(text))\n",
    "    stop_words = set(stopwords.words('russian'))\n",
    "    words = [word for word in words if word and word not in stop_words]\n",
    "    n_grams = ngrams(words, n)\n",
    "    return Counter(n_grams)\n",
    "\n",
    "# Анализ частей речи (весь текст)\n",
    "def pos_analysis(text):\n",
    "    doc = nlp(text)\n",
    "    pos_counts = Counter([token.pos_ for token in doc])\n",
    "    return pos_counts\n",
    "\n",
    "# Длина предложений\n",
    "def sentence_length_analysis(text):\n",
    "    sentences = sent_tokenize(text)\n",
    "    lengths = [len(word_tokenize(sent)) for sent in sentences]\n",
    "    return sum(lengths) / len(lengths) if lengths else 0, lengths\n",
    "\n",
    "# Нечёткое сравнение\n",
    "def fuzzy_comparison(text1, text2, sample_size=1000):\n",
    "    text1_chunk = clean_text(text1)[:sample_size]\n",
    "    text2_chunk = clean_text(text2)[:sample_size]\n",
    "    return fuzz.ratio(text1_chunk, text2_chunk)\n",
    "\n",
    "# Вероятность единого авторства\n",
    "def authorship_probability(fuzzy_score, accuracy, common_words, total_words1, total_words2, avg_len1, avg_len2):\n",
    "    fuzzy_weight = fuzzy_score / 100  # 30% веса\n",
    "    classifier_weight = (1 - accuracy) * 0.5 + 0.5  # 20% веса\n",
    "    common_words_ratio = common_words / min(total_words1, total_words2)  # 40% веса\n",
    "    len_diff = abs(avg_len1 - avg_len2) / max(avg_len1, avg_len2)\n",
    "    len_weight = 1 - min(len_diff, 1)  # 10% веса\n",
    "    probability = (fuzzy_weight * 0.3 + classifier_weight * 0.2 + common_words_ratio * 0.4 + len_weight * 0.1)\n",
    "    return min(max(probability, 0), 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Полный анализ и вывод вероятности"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "462713f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Части речи в f:\\chatepc\\chatalx\\work\\data\\obriv.txt:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Часть речи</th>\n",
       "      <th>Частота</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NOUN</td>\n",
       "      <td>42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>PUNCT</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ADJ</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>VERB</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>ADP</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>ADV</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NUM</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>PROPN</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>CCONJ</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>SPACE</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>AUX</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>DET</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>SCONJ</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>PART</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>PRON</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Часть речи  Частота\n",
       "0        NOUN       42\n",
       "8       PUNCT       30\n",
       "2         ADJ       21\n",
       "4        VERB       16\n",
       "5         ADP       16\n",
       "6         ADV       12\n",
       "3         NUM       11\n",
       "7       PROPN        9\n",
       "10      CCONJ        9\n",
       "1       SPACE        6\n",
       "9         AUX        5\n",
       "12        DET        4\n",
       "13      SCONJ        4\n",
       "14       PART        4\n",
       "11       PRON        2"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Части речи в f:\\chatepc\\chatalx\\work\\data\\oblomov.txt:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Часть речи</th>\n",
       "      <th>Частота</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NOUN</td>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>PUNCT</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ADP</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>ADJ</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>VERB</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>PROPN</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>CCONJ</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>SPACE</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>ADV</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NUM</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>DET</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>PART</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>PRON</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>AUX</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>SCONJ</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Часть речи  Частота\n",
       "4        NOUN       49\n",
       "6       PUNCT       27\n",
       "2         ADP       24\n",
       "5         ADJ       15\n",
       "8        VERB       14\n",
       "0       PROPN       10\n",
       "11      CCONJ       10\n",
       "1       SPACE        7\n",
       "12        ADV        7\n",
       "3         NUM        6\n",
       "10        DET        6\n",
       "13       PART        5\n",
       "7        PRON        4\n",
       "9         AUX        3\n",
       "14      SCONJ        1"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Пути к файлам\n",
    "file1 = \"f:\\\\chatepc\\\\chatalx\\\\work\\\\data\\\\obriv.txt\"\n",
    "file2 = \"f:\\\\chatepc\\\\chatalx\\\\work\\\\data\\\\oblomov.txt\"\n",
    "\n",
    "# Чтение текстов\n",
    "text1 = read_file(file1)\n",
    "text2 = read_file(file2)\n",
    "print(f\"Первые 100 символов {file1}: {text1[:100]}\")\n",
    "print(f\"Первые 100 символов {file2}: {text2[:100]}\")\n",
    "\n",
    "# 1. Частотность слов\n",
    "freq1 = word_frequency(text1)\n",
    "freq2 = word_frequency(text2)\n",
    "top_words1 = pd.DataFrame(freq1.most_common(10), columns=['Слово', 'Частота'])\n",
    "top_words2 = pd.DataFrame(freq2.most_common(10), columns=['Слово', 'Частота'])\n",
    "print(f\"Топ-10 слов в {file1}:\")\n",
    "display(top_words1)\n",
    "print(f\"Топ-10 слов в {file2}:\")\n",
    "display(top_words2)\n",
    "common_words = len(set(freq1.keys()) & set(freq2.keys()))\n",
    "print(f\"Количество общих слов: {common_words}\")\n",
    "\n",
    "# 2. Биграммы\n",
    "bigrams1 = ngram_frequency(text1, n=2)\n",
    "bigrams2 = ngram_frequency(text2, n=2)\n",
    "top_bigrams1 = pd.DataFrame([( ' '.join(bg), freq) for bg, freq in bigrams1.most_common(5)], columns=['Биграмма', 'Частота'])\n",
    "top_bigrams2 = pd.DataFrame([( ' '.join(bg), freq) for bg, freq in bigrams2.most_common(5)], columns=['Биграмма', 'Частота'])\n",
    "print(f\"Топ-5 биграмм в {file1}:\")\n",
    "display(top_bigrams1)\n",
    "print(f\"Топ-5 биграмм в {file2}:\")\n",
    "display(top_bigrams2)\n",
    "\n",
    "# 3. Анализ частей речи (весь текст)\n",
    "print(\"Анализ частей речи может занять несколько минут...\")\n",
    "pos1 = pos_analysis(text1)  # Обрабатываем весь текст\n",
    "pos2 = pos_analysis(text2)\n",
    "total_pos1 = sum(pos1.values())\n",
    "total_pos2 = sum(pos2.values())\n",
    "pos_df1 = pd.DataFrame(\n",
    "    [(pos, freq, freq / total_pos1 * 100) for pos, freq in pos1.items()],\n",
    "    columns=['Часть речи', 'Частота', 'Процент']\n",
    ").sort_values(by='Частота', ascending=False)\n",
    "pos_df2 = pd.DataFrame(\n",
    "    [(pos, freq, freq / total_pos2 * 100) for pos, freq in pos2.items()],\n",
    "    columns=['Часть речи', 'Частота', 'Процент']\n",
    ").sort_values(by='Частота', ascending=False)\n",
    "print(f\"Части речи в {file1} (весь текст):\")\n",
    "display(pos_df1)\n",
    "print(f\"Части речи в {file2} (весь текст):\")\n",
    "display(pos_df2)\n",
    "\n",
    "# 4. Длина предложений\n",
    "avg_len1, lengths1 = sentence_length_analysis(text1)\n",
    "avg_len2, lengths2 = sentence_length_analysis(text2)\n",
    "print(f\"Средняя длина предложений в {file1}: {avg_len1:.2f} слов\")\n",
    "print(f\"Средняя длина предложений в {file2}: {avg_len2:.2f} слов\")\n",
    "\n",
    "# 5. Нечёткое сходство\n",
    "fuzzy_score = fuzzy_comparison(text1, text2)\n",
    "\n",
    "# 6. Классификатор Naive Bayes\n",
    "vectorizer = TfidfVectorizer(max_features=2000, ngram_range=(1, 2))\n",
    "X = vectorizer.fit_transform([text1, text2])\n",
    "sentences1 = sent_tokenize(text1)\n",
    "sentences2 = sent_tokenize(text2)\n",
    "X_sentences = vectorizer.transform(sentences1 + sentences2)\n",
    "y_sentences = [0] * len(sentences1) + [1] * len(sentences2)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_sentences, y_sentences, test_size=0.3, random_state=42, stratify=y_sentences)\n",
    "clf = MultinomialNB()\n",
    "clf.fit(X_train, y_train)\n",
    "accuracy = clf.score(X_test, y_test)\n",
    "scores = cross_val_score(clf, X_sentences, y_sentences, cv=5)\n",
    "\n",
    "# 7. Итоговая вероятность\n",
    "total_words1 = len(set(word_tokenize(clean_text(text1))))\n",
    "total_words2 = len(set(word_tokenize(clean_text(text2))))\n",
    "prob = authorship_probability(fuzzy_score, accuracy, common_words, total_words1, total_words2, avg_len1, avg_len2)\n",
    "\n",
    "# 8. Таблица итоговых метрик\n",
    "results_df = pd.DataFrame({\n",
    "    'Метрика': ['Нечёткое сходство (%)', 'Точность Naive Bayes (тест)', 'Средняя точность (кросс-валидация)', 'Вероятность единого авторства (%)'],\n",
    "    'Значение': [fuzzy_score, f\"{accuracy:.2f}\", f\"{scores.mean():.2f} (±{scores.std():.2f})\", f\"{prob:.2%}\"]\n",
    "})\n",
    "print(\"\\nИтоговые метрики анализа:\")\n",
    "display(results_df)\n",
    "\n",
    "# Визуализация\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(lengths1, bins=20, alpha=0.5, label=file1)\n",
    "plt.hist(lengths2, bins=20, alpha=0.5, label=file2)\n",
    "plt.legend(loc='upper right')\n",
    "plt.title('Распределение длины предложений')\n",
    "plt.xlabel('Количество слов')\n",
    "plt.ylabel('Частота')\n",
    "plt.show()\n",
    "\n",
    "pos1_labels, pos1_values = zip(*pos1.items())\n",
    "pos2_labels, pos2_values = zip(*pos2.items())\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.bar(pos1_labels, pos1_values, alpha=0.5, label=file1)\n",
    "plt.bar(pos2_labels, pos2_values, alpha=0.5, label=file2)\n",
    "plt.legend(loc='upper right')\n",
    "plt.title('Распределение частей речи')\n",
    "plt.xlabel('Часть речи')\n",
    "plt.ylabel('Частота')\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f96e14e9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
